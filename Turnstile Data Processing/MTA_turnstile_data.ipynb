{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MTA_turnstile_data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN8SH9UQIZwjY9af3sGqklz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CUSPADS2022IBX/IBXRidership/blob/main/Turnstile%20Data%20Processing/MTA_turnstile_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Source: http://web.mta.info/developers/turnstile.html\n",
        "\n",
        "Example:\n",
        "\n",
        "The data below shows the entry/exit register values for one turnstile at control area (A002) from 09/27/14 at 00:00 hours to 09/29/14 at 00:00 hours\n",
        "\n",
        "Schema Example:\n",
        "C/A,UNIT,SCP,STATION,LINENAME,DIVISION,DATE,TIME,DESC,ENTRIES,EXITS\n",
        "A002,R051,02-00-00,LEXINGTON AVE,456NQR,BMT,09-27-14,00:00:00,REGULAR,0004800073,0001629137,\n",
        "\n",
        "Data cleaning and processing resources used:\n",
        "\n",
        "1)https://medium.com/qri-io/taming-the-mtas-unruly-turnstile-data-c945f5f96ba0\n",
        "\n",
        "2)https://toddwschneider.com/dashboards/nyc-subway-turnstiles/#notes\n"
      ],
      "metadata": {
        "id": "8HzqGRmo_BsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "!pip install --upgrade xlrd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZalDTGmcDQmE",
        "outputId": "04464bef-8996-439b-aa28-d37e95cb1be7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 36 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 45.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=8a89c24215c9d53902e429ee348d76438e6d6676bea00bf0e194becda87ad0c3\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Collecting xlrd\n",
            "  Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 3.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: xlrd\n",
            "  Attempting uninstall: xlrd\n",
            "    Found existing installation: xlrd 1.1.0\n",
            "    Uninstalling xlrd-1.1.0:\n",
            "      Successfully uninstalled xlrd-1.1.0\n",
            "Successfully installed xlrd-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "ijUc954X941k"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, date, timedelta\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import types as T\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark import SparkFiles\n",
        "\n",
        "sc = pyspark.SparkContext.getOrCreate()\n",
        "spark = SparkSession(sc)\n",
        "\n",
        "#The start date has to match the date of the first URL in the MTA turnstile data, otherwise the URL pull will not work. Data is reported every Saturday.\n",
        "#NOTE: On November 20, 2021 MTA changed their normal turnstile count periods from [12AM, 4AM, 8AM, 12PM, 4PM, 8PM] to [3AM, 7AM, 11AM, 3PM, 7PM, 11PM]\n",
        "start_date = date(2021,11, 20)\n",
        "end_date = date(2022, 4, 9)\n",
        "\n",
        "#Create a list of dates for the date range requested\n",
        "date_range = list(pd.date_range(start_date, end_date, freq='7D').strftime(\"%y%m%d\"))\n",
        "\n",
        "#MTA tunrstile schema, 'EXITS' kepts giving nulls when imported as IntegerType\n",
        "mta_turnstile_schema = T.StructType([\n",
        "  T.StructField('C/A', T.StringType(), True),\n",
        "  T.StructField('UNIT', T.StringType(), True),\n",
        "  T.StructField('SCP', T.StringType(), True),\n",
        "  T.StructField('STATION', T.StringType(), True),\n",
        "  T.StructField('LINENAME', T.StringType(), True),\n",
        "  T.StructField('DIVISION', T.StringType(), True),\n",
        "  T.StructField('DATE', T.StringType(), True),\n",
        "  T.StructField('TIME', T.StringType(), True),\n",
        "  T.StructField('DESC', T.StringType(), True),\n",
        "  T.StructField('ENTRIES', T.IntegerType(), True),\n",
        "  T.StructField('EXITS', T.FloatType(), True),\n",
        "  ])\n",
        "\n",
        "#Create empty dataframe with previous scheme\n",
        "bigdf = spark.createDataFrame([], mta_turnstile_schema)\n",
        "\n",
        "#Download each .txt file on to Spark job node and load into Spark DataFrame and union onto Empty DataFrame we created\n",
        "for date_string in date_range:\n",
        "  url = 'http://web.mta.info/developers/data/nyct/turnstile/turnstile_{}.txt'.format(date_string)\n",
        "  spark.sparkContext.addFile(url)\n",
        "  df = spark.read.csv(SparkFiles.get('turnstile_{}.txt'.format(date_string)), mta_turnstile_schema, header=True)\n",
        "  bigdf = bigdf.union(df)\n",
        "\n",
        "#Change 'EXITS' column data type to IntegerType and concate 'DATE' and 'TIME' columns and cast to datetime\n",
        "bigdf = bigdf.withColumn('EXITS',bigdf.EXITS.cast(T.IntegerType()))\\\n",
        "             .withColumn('timestamp',\n",
        "                         F.unix_timestamp(F.concat(bigdf.DATE,bigdf.TIME),'MM/dd/yyyyHH:mm:ss').cast('timestamp'))\n",
        "\n",
        "#Create columns to represent unique observation id, and unique turnstile id for data processing             \n",
        "bigdf = bigdf.withColumn('unit_division', F.concat(bigdf.UNIT,bigdf.DIVISION))\\\n",
        "             .withColumn('unit_id', F.concat(bigdf['C/A'],bigdf.UNIT,bigdf.SCP))\n",
        "\n",
        "#Use utility function window to partition by turnstile and order by timestamp\n",
        "window = Window.partitionBy('unit_id').orderBy('timestamp')\n",
        "\n",
        "#Use previous window to find the 'net_entries' and 'net_exits'. Remove all entries that are above 10000, because\n",
        "#turnstiles act as odometers, and when turnstile reaches end it resets creating a large value. 10000 is a good cutoff.\n",
        "#Also drop first rows of each turnstile data, because .lag function creates None for first row.\n",
        "bigdf = bigdf.withColumn('net_entries', F.abs(F.col('ENTRIES') - F.lag(F.col('ENTRIES'), 1).over(window)))\\\n",
        "             .withColumn('net_exits', F.abs(F.col('EXITS') - F.lag(F.col('EXITS'), 1).over(window)))\\\n",
        "             .where((F.col('net_entries')<10000) |\\\n",
        "                    (F.col('net_exits')<10000) |\\\n",
        "                    (F.col('net_entries')!=None) |\\\n",
        "                    (F.col('net_exits')!=None))\n",
        "             \n",
        "#Create new column for 'DOW' (day of week) to aggregate by weekends and weekdays.\n",
        "bigdf = bigdf.withColumn('DOW', F.when((F.dayofweek(F.col('timestamp'))<7) & (F.dayofweek(F.col('timestamp'))>1),'weekday')\\\n",
        "                        .when((F.dayofweek(F.col('timestamp'))==7) | (F.dayofweek(F.col('timestamp'))==1),'weekend'))\n",
        "\n",
        "#Creates new column 'TOD' (Time of Day)\n",
        "#NOTE: If you want to analyze data before 11/20/2021 you will have to agregate at different hour intervals\n",
        "bigdf = bigdf.withColumn('TOD', F.when((F.date_format(F.col('timestamp'), 'HH:mm:ss')> '23:00:00') | (F.date_format(F.col('timestamp'), 'HH:mm:ss')<= '07:00:00'), 'overnight')\\\n",
        "                        .when((F.date_format(F.col('timestamp'), 'HH:mm:ss')> '07:00:00') & (F.date_format(F.col('timestamp'), 'HH:mm:ss')<= '15:00:00'), 'morning')\\\n",
        "                        .when((F.date_format(F.col('timestamp'), 'HH:mm:ss')> '11:00:00') & (F.date_format(F.col('timestamp'), 'HH:mm:ss')<= '23:00:00'), 'evening'))\n",
        "\n",
        "#Upload Remote_complex_lookup table and create key table for unit_division join\n",
        "#Manually checked if complex_id was correct (google sheets for reference: https://docs.google.com/spreadsheets/d/1kMmoqzq3uWM5J8Esrzi1DPEBrdezzVtQ1Rv5ZAIsEfk/edit?usp=sharing)\n",
        "remote_complex_url = 'https://raw.githubusercontent.com/qri-io/data-stories-scripts/master/nyc-turnstile-counts/lookup/remote_complex_lookup.csv'\n",
        "remote_complex = pd.read_csv(remote_complex_url).sort_values('station')\n",
        "remote_complex['complex_id'] = remote_complex['complex_id'].astype('Int64').astype('str')\n",
        "remote_complex['unit_division'] = remote_complex['remote ']+remote_complex['division']\n",
        "remote_complex_spark = spark.createDataFrame(remote_complex)\n",
        "\n",
        "#join to the bigdf to create unique complex_Id column to aggregate on\n",
        "bigdf = bigdf.join(remote_complex_spark, bigdf.unit_division==remote_complex_spark.unit_division, 'left')\\\n",
        "        .select('net_entries','net_exits','DOW','TOD','complex_id').dropna()\n",
        "\n",
        "#Aggregate on complex_id, weekend/weekday statu, Morning, Evening, Overnight\n",
        "entries_exits_df = bigdf.groupBy('DOW','TOD','complex_id').agg({'net_entries':'avg', 'net_exits':'avg'}).sort(F.col('complex_ID'))\n",
        "\n",
        "weekend_morning = entries_exits_df.filter((entries_exits_df.DOW == 'weekend') & (entries_exits_df.TOD == 'morning')).toPandas()\n",
        "weekend_evening = entries_exits_df.filter((entries_exits_df.DOW == 'weekend') & (entries_exits_df.TOD == 'evening')).toPandas()\n",
        "weekend_overnight = entries_exits_df.filter((entries_exits_df.DOW == 'weekend') & (entries_exits_df.TOD == 'overnight')).toPandas()\n",
        "weekday_morning = entries_exits_df.filter((entries_exits_df.DOW == 'weekday') & (entries_exits_df.TOD == 'morning')).toPandas()\n",
        "weekday_evening = entries_exits_df.filter((entries_exits_df.DOW == 'weekday') & (entries_exits_df.TOD == 'evening')).toPandas()\n",
        "weekday_orvernight = entries_exits_df.filter((entries_exits_df.DOW == 'weekday') & (entries_exits_df.TOD == 'overnight')).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weekend_evening.loc[:425].to_csv('weekend_evening.csv')\n",
        "weekend_morning.loc[:425].to_csv('weekend_morning.csv')\n",
        "weekend_overnight.loc[:425].to_csv('weekend_overnight.csv')\n",
        "weekday_morning.loc[:425].to_csv('weekday_morning.csv')\n",
        "weekday_evening.loc[:425].to_csv('weekday_evening.csv')\n",
        "weekday_orvernight.loc[:425].to_csv('weekday_overnight.csv')"
      ],
      "metadata": {
        "id": "gnQdkpiz72bf"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "turnstile_stations = list(bigdf.select(['STATION']).distinct().sort(F.col('STATION')).toPandas()['STATION'])\n",
        "turnstile_stations"
      ],
      "metadata": {
        "id": "SkZbk-5Ysxbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "turnstile_station_unit= bigdf.select(['STATION', 'UNIT', 'DIVISION']).distinct().sort(F.col('STATION')).toPandas()\n",
        "turnstile_station_unit.to_csv('turnstile_stations.csv')"
      ],
      "metadata": {
        "id": "EQIWjD6jL1yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stations_url = 'http://web.mta.info/developers/data/nyct/subway/Stations.csv'\n",
        "\n",
        "stations = pd.read_csv(stations_url).sort_values('Stop Name')\n",
        "\n",
        "#print(stations.groupby(['Complex ID','Stop Name','GTFS Stop ID','Division']).size().reset_index().rename(columns={0:'count'}))\n",
        "stations['Stop Name'] =  stations['Stop Name'].str.upper()\n",
        "#stations['Complex ID', 'GTFS Stop ID','Stop Name'].value_counts()\n",
        "\n",
        "stations.to_csv('stations_list.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IefF8iKI7GmB",
        "outputId": "f0fb8203-61f5-4a64-986d-5191a73392ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Complex ID                 Stop Name GTFS Stop ID Division  count\n",
            "0             1      Astoria-Ditmars Blvd          R01      BMT      1\n",
            "1             2              Astoria Blvd          R03      BMT      1\n",
            "2             3                     30 Av          R04      BMT      1\n",
            "3             4                  Broadway          R05      BMT      1\n",
            "4             5                     36 Av          R06      BMT      1\n",
            "..          ...                       ...          ...      ...    ...\n",
            "491         630        Myrtle-Wyckoff Avs          M08      BMT      1\n",
            "492         635               South Ferry          142      IRT      1\n",
            "493         635  Whitehall St-South Ferry          R27      BMT      1\n",
            "494         636          Jay St-MetroTech          A41      IND      1\n",
            "495         636          Jay St-MetroTech          R29      BMT      1\n",
            "\n",
            "[496 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stations_list = stations.groupby(['Complex ID','Stop Name','GTFS Stop ID', 'Division']).size().reset_index().rename(columns={0:'count'})\n",
        "stations_list.to_csv('stations_list.csv')"
      ],
      "metadata": {
        "id": "byHTmWPWPXnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_name = list(stations[['Stop Name','GTFS Stop ID','Complex ID']].unique())\n",
        "stop_name"
      ],
      "metadata": {
        "id": "u57TZtKIMqGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "turnstile_key_url = 'http://web.mta.info/developers/resources/nyct/turnstile/Remote-Booth-Station.xls'\n",
        "\n",
        "turnstile_key = pd.read_excel(turnstile_key_url)\n",
        "pd.set_option('display.max_rows', turnstile_key.shape[0]+1)\n",
        "\n",
        "turnstile_key.head()\n",
        "print(turnstile_key.groupby(['Remote','Station']).size().reset_index().rename(columns={0:'count'}))"
      ],
      "metadata": {
        "id": "tnzU1Q9PhQFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "remote_complex_spark\n",
        "#len(remote_complex['station'].unique())\n",
        "#station_list = list(remote_complex['station'].unique())\n",
        "#print(remote_complex.groupby(['complex_id','station']).size().reset_index().rename(columns={0:'count'}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zx762D1x55UC",
        "outputId": "b97332ea-2c8b-49ad-a0de-6383ad0bd509"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[remote : string, booth: string, complex_id: string, station: string, line_name: string, division: string, unit_division: string]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Px_Z86ZEQhPb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}